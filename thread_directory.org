#+PROPERTY: header-args:jupyter-python  :session py
#+PROPERTY: header-args    :pandoc t

* Updating the thread directory
We want to be able to automatically update the thread directory in r/c.

The thread directory is a markdown page with several tables of data recording information about each active /r/c counting thread, in the following format:

| Name & First thread | Latest comment | Total comments |

When updating the directory, the first column does not change, only the second and the third do

In order to update a row in a single table table, we need to [[Parsing][parse]] the existing row, find the [[Getting threads][correct reddit submission]] of the latest count, and find the [[Finding the latest comment][latest comment in the thread]]. Finally, we need to print the row in the correct format

** Imports and initialisation
#+begin_src jupyter-python
  import praw
  import re
  import pandas as pd
  import numpy as np
  import datetime
  from pprint import pprint
  import configparser
  from side_threads import get_side_thread
  from parsing import find_urls_in_text, parse_markdown_links
  from thread_navigation import fetch_comment_tree, walk_down_thread, fetch_thread
  from utils import flatten
  from itertools import chain
  
  r = praw.Reddit('stats_bot')
  subreddit = r.subreddit('counting')
#+end_src

** Parsing
We can get the markdown of the file using the `content_md` attribute of the wiki page. Sometimes the lines are windows terminated, so we replace those with regular newlines
#+begin_src jupyter-python
  directory_page = subreddit.wiki['directory'].content_md
  directory_page = directory_page.replace("\r\n", "\n")
  
#+end_src

The file consists of a header and a footer. In the middle, there are headers and tables separated by blank lines. We split the file into paragraphs, and then process each paragraph one at a time. That way, we can (In theory) just glue everything together with double newlines when we're done

We process paragraph by paragraph, and the criterion for finding a table is a whole paragraph that matches the regex.
#+begin_src jupyter-python
  paragraphs = directory_page.split("\n\n")
  regex = r"^.*\|.*\|.*$"
  tagged_results = []
  for paragraph in paragraphs:
      lines = paragraph.split("\n")
      mask = np.all([bool(re.match(regex, line)) for line in lines])
      if not mask:
          tagged_results.append(['text', paragraph])
      else:
          tagged_results.append(['table', [x.split('|') for x in lines]])
  print(len(tagged_results))
  
        #+end_src
The result of all this is a tagged list, with the text portion saved as it originally was under the 'text' tag, and the tables stored as nested lists and tagged as 'table'.
** Getting threads
We get all threads from the last six months:

#+begin_src jupyter-python
  six_months = datetime.timedelta(weeks=26.5)
  now = datetime.datetime.utcnow()
  posts = r.subreddit('counting').new(limit=1000)
  tidbits_regex = "(T|t)idbits"
  ftf_regex = "free talk friday"
  tree = {}
  posts_dict = {}
  new_threads = []
  weird_threads = ['nehhvf']
  for post in posts:
      if post.id in weird_threads:
          continue
      posts_dict[post.id] = post
      title = post.title
      if re.match(tidbits_regex, title) or re.match(ftf_regex, title.lower()):
          continue
      body = post.selftext
      try:
          body_urls = find_urls_in_text(body)
          post_urls = flatten([find_urls_in_text(comment.body) for comment in post.comments])
          urls = chain(body_urls, post_urls)
          urls = filter(lambda x: int(x[0], 36) < int(post.id, 36), urls)
          url = next(urls)
          tree[post.id] = url[0]
      except StopIteration:
          new_threads.append(post.id)
      post_time  = datetime.datetime.utcfromtimestamp(post.created_utc)
      if now - post_time > six_months:
          break
  else:  # no break
      print('Threads between {now - six_months} and {post_time} have not been collected')
  
#+end_src

#+begin_src jupyter-python
  child_tree  = {v: k for k, v in tree.items()}
  def find_leaf_thread(submission_id):
      if submission_id not in posts_dict and submission_id not in child_tree:
          # Post has been archived with no children
          return None
      while submission_id in child_tree:
          submission_id = child_tree[submission_id]
      return submission_id
  
#+end_src
We check that everything works by looking up the leaf thread for all of our tables. Threads which are leaves, but have been archived are separated out.

#+begin_src jupyter-python
  counter = 1
  latest_threads = {}
  archived_threads = []
  for idx, entry in enumerate(tagged_results):
      if entry[0] == "text":
          continue
      elif entry[0] == "table":
          table = entry[1][2:]
          for line in table:
              counter += 1
              thread_id, comment_id = find_urls_in_text(line[1])[0]
              if thread_id in new_threads:
                  new_threads.remove(thread_id)
              leaf_thread = find_leaf_thread(thread_id)
              if leaf_thread is not None:
                  if leaf_thread == thread_id:
                      latest_threads[thread_id] = [leaf_thread, comment_id]
                  else:
                      latest_threads == [leaf_thread, None]
              else:
                  archived_threads.append([thread_id, comment_id])
#+end_src

** Finding the latest comment
Finding the latest comment is incredibly tricky to do robustly, because people aren't robots and mistakes are made. Ideally, we need to account for

- Late counts
- Early counts
- Random conversation happening in the middle of a thread

Solving this generally is not possible. Instead, I'll use a heuristic approach.

Let the "root node" be either the latest logged comment in the thread if no new submissions have been posted of this type since the directory was last updated, and let it be the first valid comment in the thread otherwise. Then we walk down the thread from the root comment, taking the first valid comment every time. We keep going until a comment that looks like a count has no children that look like counts; that's the comment we want.  

This requires a per-thread `valid counts` rule to specify what looks like a count. These rules should be designed permissively, so that even slightly wonky counts seem to look like counts.

The rules for a side thread are things like:

- An "is valid count rule", which validates whether a certain count fulfils the rules of the thread (e.g slow or wait 2)
- A "get_history(comment)" function which returns enough history that it's possible to determine whether `comment` is valid
- A "looks like a count" rule, which helps decide whether we've hit random conversation
- A "thread length" parameter, which determines how long each thread (normally) is in this side thread
- Others?

These bits of information are now stored in a rules object for each side thread, which contains the necessary properties.

A table of currently known side threads is found in [[file:side_threads.ini][side_threads.ini]], while [[file:side_threads.py][side_threads.py]] has a lookup function for each known side thread, returning a side thread object. We can try validating every entry in the directory table and see that everything works:
#+begin_src jupyter-python
  import configparser
  config = configparser.ConfigParser()
  config.read('side_threads.ini')
  side_threads = config['threads']
  side_threads['5c2d55']

#+end_src

#+begin_src jupyter-python
  side_thread = get_side_thread(side_threads['5c2d55'])
  comment_id = 'h1ehq8f'
  comment = r.comment(comment_id)
  tree = fetch_comment_tree(thread=comment.submission, root=comment)
  comment = tree.comment(comment_id)

  walk_down_thread(side_thread, comment)

#+end_src
#+begin_src jupyter-python
comment.to_dict()
#+end_src

** Printing the output


