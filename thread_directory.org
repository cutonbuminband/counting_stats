#+PROPERTY: header-args:jupyter-python  :session py
#+PROPERTY: header-args    :pandoc t

* Updating the thread directory
We want to be able to automatically update the thread directory in r/c.

The thread directory is a markdown page with several tables of data recording information about each active /r/c counting thread, in the following format:

| Name & First thread | Latest comment | Total comments |

When updating the directory, the first column does not change, only the second and the third do

In order to update a row in a single table table, we need to [[Parsing][parse]] the existing row, find the [[Getting threads][correct reddit submission]] of the latest count, and find the [[Finding the latest comment][latest comment in the thread]]. Finally, we need to print the row in the correct format

** Imports and initialisation
#+begin_src jupyter-python
import praw
import re
import numpy as np
import datetime
import configparser
from side_threads import get_side_thread
from parsing import find_urls_in_text, parse_markdown_links
from thread_navigation import fetch_comment_tree, walk_down_thread
from utils import flatten
from itertools import chain
  
r = praw.Reddit('stats_bot')
subreddit = r.subreddit('counting')
#+end_src

** Parsing
We can get the markdown of the file using the `content_md` attribute of the wiki page. Sometimes the lines are windows terminated, so we replace those with regular newlines
#+begin_src jupyter-python
directory_page = subreddit.wiki['directory'].content_md
directory_page = directory_page.replace("\r\n", "\n")
#+end_src

The file consists of a header and a footer. In the middle, there are headers and tables separated by blank lines. We split the file into paragraphs, and then process each paragraph one at a time. That way, we can (In theory) just glue everything together with double newlines when we're done

We process paragraph by paragraph, and the criterion for finding a table is a whole paragraph that matches the regex.
#+begin_src jupyter-python
paragraphs = directory_page.split("\n\n")
regex = r"^.*\|.*\|.*$"
tagged_results = []
for paragraph in paragraphs:
    lines = paragraph.split("\n")
    mask = np.all([bool(re.match(regex, line)) for line in lines])
    if not mask:
        tagged_results.append(['text', paragraph])
    else:
        tagged_results.append(['table', [x.split('|') for x in lines]])
print(len(tagged_results))
  
        #+end_src

The result of all this is a tagged list, with the text portion saved as it originally was under the 'text' tag, and the tables stored as nested lists and tagged as 'table'.
** Getting threads
We get all threads from the last six months:

#+begin_src jupyter-python
six_months = datetime.timedelta(weeks=26.5)
now = datetime.datetime.utcnow()
posts = r.subreddit('counting').new(limit=1000)
tidbits_regex = "(T|t)idbits"
ftf_regex = "free talk friday"
tree = {}
posts_dict = {}
new_threads = []
weird_threads = ['nehhvf']
for post in posts:
    if post.id in weird_threads:
        continue
    posts_dict[post.id] = post
    title = post.title
    if re.match(tidbits_regex, title) or re.match(ftf_regex, title.lower()):
        continue
    body = post.selftext
    try:
        body_urls = find_urls_in_text(body)
        post_urls = flatten([find_urls_in_text(comment.body) for comment in post.comments])
        urls = chain(body_urls, post_urls)
        urls = filter(lambda x: int(x[0], 36) < int(post.id, 36), urls)
        url = next(urls)
        tree[post.id] = url[0]
    except StopIteration:
        new_threads.append(post.id)
    post_time  = datetime.datetime.utcfromtimestamp(post.created_utc)
    if now - post_time > six_months:
        break
else:  # no break
    print('Threads between {now - six_months} and {post_time} have not been collected')

  
#+end_src

Reversing the parent->child relation gives us a way of walking down the chain of threads to the leaf thread
#+begin_src jupyter-python
child_tree  = {v: k for k, v in tree.items()}
def walk_down_thread_chain(submission_id):
    result = [submission_id]
    if submission_id not in posts_dict and submission_id not in child_tree:
        # Post has been archived with no children
        return None
    while submission_id in child_tree:
        submission_id = child_tree[submission_id]
        result.append(submission_id)
    return result

#+end_src

For each comment in the (potential) new threads, we walk down the chain to the leaf. Later, we'll compare this value with the leaves of already known threads, and see if there's a match.

#+begin_src jupyter-python
new_threads = [walk_down_thread_chain(x)[-1] for x in new_threads]
#+end_src

** Finding the latest comment
Finding the latest comment is incredibly tricky to do robustly, because people aren't robots and mistakes are made. Ideally, we need to account for

- Late counts
- Early counts
- Random conversation happening in the middle of a thread

Solving this generally is not possible. Instead, I'll use a heuristic approach.

Let the "root node" be either the latest logged comment in the thread if no new submissions have been posted of this type since the directory was last updated, and let it be the first valid comment in the thread otherwise. Then we walk down the thread from the root comment, taking the first valid comment every time. We keep going until a comment that looks like a count has no children that look like counts; that's the comment we want.  

This requires a per-thread `valid counts` rule to specify what looks like a count. These rules should be designed permissively, so that even slightly wonky counts seem to look like counts.

The rules for a side thread are things like:

- An "is valid count rule", which validates whether a certain count fulfils the rules of the thread (e.g slow or wait 2)
- A "get_history(comment)" function which returns enough history that it's possible to determine whether `comment` is valid
- A "looks like a count" rule, which helps decide whether we've hit random conversation
- A "thread length" parameter, which determines how long each thread (normally) is in this side thread
- Others?

These bits of information are now stored in a rules object for each side thread, which contains the necessary properties.

A table of currently known side threads is found in [[file:side_threads.ini][side_threads.ini]], while [[file:side_threads.py][side_threads.py]] has a lookup function for each known side thread, returning a side thread object. We can try validating every entry in the directory table and see that everything works:
#+begin_src jupyter-python
config = configparser.ConfigParser()
config.read('side_threads.ini')
known_side_threads = config['threads']
#+end_src

** Putting it all together
to update a single row, we should do the following:
- Leave the first field unchanged
- Find the latest thread and latest comment, and link to that in the second field
- Update the total number of counts
- If the thread has been archived, flag that.

#+begin_src jupyter-python
def update_row(row, verbose=True):
    first, current, count = row
    thread_name, first_thread = parse_markdown_links(first)[0]
    first_thread = first_thread[1:]
    previous_thread, previous_comment = find_urls_in_text(current)[0]
    thread_chain = walk_down_thread_chain(previous_thread)
    is_archived = False
    if thread_chain is None:
        is_archived = True
        thread_chain = [previous_thread]
    latest_thread = thread_chain[-1]
    praw_thread = r.submission(latest_thread)
    if latest_thread != previous_thread or not previous_comment:
            previous_comment = praw_thread.comments[0].id
    comment_tree = fetch_comment_tree(praw_thread, root_id=previous_comment)
    thread_name = known_side_threads.get(first_thread, fallback='decimal')
    side_thread = get_side_thread(thread_name)
    new_comment = walk_down_thread(side_thread, comment_tree.comment(previous_comment))
    new_title = praw_thread.title.split("|")[-1]
    new_link = f'[{new_title}](reddit.com/r/comments/{latest_thread}/_/{new_comment.id})'
    try:
        old_count = int(count.translate(str.maketrans('-', '0', ', ')))
        new_count = side_thread.update_count(old_count, thread_chain)
        new_count = f"{new_count:,}"
    except (ValueError, TypeError):
        new_count = f"{count}*"
    new_row = [first, new_link, new_count]
    if verbose:
        print(thread_name)
    return is_archived, new_row, latest_thread
#+end_src

We can try on our table object
#+begin_src jupyter-python
result = []
archived_threads = []
start = datetime.datetime.now()
for idx, entry in enumerate(tagged_results):
    if entry[0] == "text":
        result.append(entry)
    elif entry[0] == "table":
        table = entry[1][2:]
        new_table = []
        for row in table:
            is_archived, new_row, latest_thread = update_row(row)
            if latest_thread in new_threads:
                new_threads.remove(latest_thread)
            if is_archived:
                archived_threads.append(new_row)
                continue
            new_table.append(new_row)
        result.append(['table', new_table])
print(datetime.datetime.now() - start)
#+end_src

** Printing the output
Now we just need to reverse the transformation we applied at the start:

Given a list of tagged entries, we need to regenerate a markdown file.

For the text paragraphs, nothing changes.
For the tables, we should separate each field by "|", and each line by "\n", and then add back the header

#+begin_src jupyter-python
table_header = [['Name &amp; Initial Thread', 'Current Thread', '# of Counts'],
                [':--:', ':--:', '--:']]
def stringify(paragraph):
    if paragraph[0] == "text":
        return paragraph[1]
    elif paragraph[0] == "table":
        return '\n'.join('|'.join(x) for x in table_header + paragraph[1])

with open("new_directory_file.md", "w") as f:
    print(*[stringify(entry) for entry in result], file=f, sep="\n\n")


with open("archived_threads.md", "w") as f:
    print(stringify(["table", archived_threads]), file=f)

print(*[f"New thread '{r.submission(x).title}' at reddit.com/comments/{x}" for x in new_threads], sep="\n")
#+end_src
