#+PROPERTY: header-args:jupyter-python  :session py
#+PROPERTY: header-args    :pandoc t

* Updating the thread directory
We want to be able to automatically update the thread directory in r/c.

The thread directory is a markdown page with several tables of data recording information about each active /r/c counting thread, in the following format:

| Name & First thread | Latest comment | Total comments |

When updating the directory, the first column does not change, only the second and the third do

In order to update a row in a single table table, we need to [[Parsing][parse]] the existing row, find the [[Getting threads][correct reddit submission]] of the latest count, and find the [[Finding the latest comment][latest comment in the thread]]. Finally, we need to print the row in the correct format

** Imports and initialisation
#+begin_src jupyter-python
  import praw
  import re
  import pandas as pd
  import numpy as np
  import datetime
  
  r = praw.Reddit('stats_bot')
  subreddit = r.subreddit('counting')
#+end_src

** Parsing
We can get the markdown of the file using the `content_md` attribute of the wiki page. Sometimes the lines are windows terminated, so we replace those with regular newlines
#+begin_src jupyter-python
  directory_page = subreddit.wiki['directory'].content_md
  directory_page = directory_page.replace("\r\n", "\n")
  directory_page = open("directory.md", "r").read()
  
#+end_src

The file consists of a header and a footer. In the middle, there are headers and tables separated by blank lines. We split the file into paragraphs, and then process each paragraph one at a time. That way, we can (In theory) just glue everything together with double newlines when we're done

We process paragraph by paragraph, and the criterion for finding a table is a whole paragraph that matches the regex.
#+begin_src jupyter-python
  paragraphs = directory_page.split("\n\n")
  regex = r"^.*\|.*\|.*$"
  tagged_results = []
  for paragraph in paragraphs:
      lines = paragraph.split("\n")
      mask = np.all([bool(re.match(regex, line)) for line in lines])
      if not mask:
          tagged_results.append(['text', paragraph])
      else:
          tagged_results.append(['table', [x.split('|') for x in lines]])
  print(len(tagged_results))
  
        #+end_src
The result of all this is a tagged list, with the text portion saved as it originally was under the 'text' tag, and the tables stored as nested lists and tagged as 'table'.
** Getting threads
We get all threads from the last six months:

#+begin_src jupyter-python
  from parsing import find_urls_in_text
  
  six_months = datetime.timedelta(weeks=26.5)
  now = datetime.datetime.utcnow()
  posts = r.subreddit('counting').new(limit=1000)
  tidbits_regex = "(T|t)idbits"
  ftf_regex = "free talk friday"
  tree = {}
  posts_dict = {}
  new_threads = []
  weird_threads = ['nehhvf']
  for post in posts:
      if post.id in weird_threads:
          continue
      posts_dict[post.id] = post
      title = post.title
      if re.match(tidbits_regex, title) or re.match(ftf_regex, title.lower()):
          continue
      body = post.selftext
      try:
          urls = find_urls_in_text(body)
          if not urls:
              urls = [find_urls_in_text(comment.body) for comment in post.comments]
              # flatten
              urls = [url for comment_urls in urls for url in comment_urls]
          url = urls[0]
          if url[0] == post.id:
              url = urls[1]
          tree[post.id] = url[0]
      except IndexError:
          if post.removed_by_category is None:
              new_threads.append(post.id)
      post_time  = datetime.datetime.utcfromtimestamp(post.created_utc)
      if now - post_time > six_months:
          break
  else:  # no break
      print('Threads between {now - six_months} and {post_time} have not been collected')
  
#+end_src

#+begin_src jupyter-python
  child_tree  = {v: k for k, v in tree.items()}
  def find_leaf_thread(submission_id):
      if submission_id not in posts_dict and submission_id not in child_tree:
          # Post has been archived with no children
          return None
      while submission_id in child_tree:
          submission_id = child_tree[submission_id]
      return submission_id
  
#+end_src
We check that everything works by looking up the leaf thread for all of our tables. Threads which are leaves, but have been archived are separated out.

#+begin_src jupyter-python
  counter = 1
  latest_threads = {}
  archived_threads = []
  for idx, entry in enumerate(tagged_results):
      if entry[0] == "text":
          continue
      elif entry[0] == "table":
          table = entry[1][2:]
          for line in table:
              counter += 1
              thread_id, comment_id = find_urls_in_text(line[1])[0]
              if thread_id in new_threads:
                  new_threads.remove(thread_id)
              leaf_thread = find_leaf_thread(thread_id)
              if leaf_thread is not None:
                  if leaf_thread == thread_id:
                      latest_threads[thread_id] = [leaf_thread, comment_id]
                  else:
                      latest_threads == [leaf_thread, None]
              else:
                  archived_threads.append([thread_id, comment_id])
    
#+end_src


** Finding the latest comment
Finding the latest comment is incredibly tricky to do robustly, because people aren't robots and mistakes are made. Ideally, we need to account for

- Late counts
- Early counts
- Invalid counts
- Random conversation happening in the middle of a thread

Solving this generally is not possible. Instead, I'll use a heuristic approach.

Let the "root node" be either the latest logged comment in the thread if no new submissions have been posted of this type since the directory was last updated, and let it be the first valid comment in the thread otherwise. Then we walk down the thread from the root comment, taking the first valid comment every time. We keep going until a comment has no children; that might be the comment we want.  

To check if it is we should do something along the lines of:

- Use a per-thread "valid counts" rule to see if it looks like it might be a count.
- If it is, we just return it.
- If it's not, or we don't have a rule for this thread, we print the body of the comment and ask the user if it looks like a valid count.
- If they say yes, we are done.
- If they say no we go up one comment and try again. If that fails, we print the url and ask the user to fix things themselves; this thread won't be updated automatically.

This works, but takes forever. Can we do something smarter?

#+begin_src jupyter-python
  from thread_navigation import psaw_get_tree, walk_down_thread
  root = r.comment('h1f7wtb')
  tree = psaw_get_tree(root.submission, root)
  print(len(tree))
#+end_src

** Printing the output


